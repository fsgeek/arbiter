System prompts for LLM-based coding agents are
software artifacts that govern agent behavior, yet lack the testing
infrastructure applied to conventional software. We present Arbiter, a
framework combining formal evaluation rules with multi-model LLM
scouring to detect interference patterns in system prompts. Applied to
three major coding agent system prompts --- Claude Code (Anthropic),
Codex CLI (OpenAI), and Gemini CLI (Google) --- we identify 152 findings
across the undirected scouring phase and 21 hand-labeled interference
patterns in directed analysis of one vendor. We show that prompt
architecture (monolithic, flat, modular) predicts the \emph{class} of
failure but not its \emph{severity}, and that multi-model evaluation
discovers categorically different vulnerability classes than
single-model analysis. One scourer finding --- structural data loss in
Gemini CLI's memory system --- was independently confirmed when Google
filed and patched the symptom without addressing the schema-level root
cause identified by the scourer. Total cost of cross-vendor analysis:
\$0.27 USD.
