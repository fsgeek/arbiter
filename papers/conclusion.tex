\section{Conclusion}

System prompts are the least-tested, most-consequential software
artifacts in modern AI systems. They govern the behavior of agents that
write code, manage files, execute shell commands, and interact with
external services. Yet they receive no static analysis, no integration
testing, and no cross-team review for internal consistency.

Three architectural patterns --- monolithic, flat, modular --- produce
three characteristic classes of failure. This taxonomy transfers from
conventional software engineering because the underlying structural
dynamics are the same: growth produces boundary contradictions,
modularity produces seam defects, simplicity buys consistency at the
cost of capability.

Multi-model evaluation discovers what single-model analysis misses.
Different LLMs bring categorically different analytical perspectives,
not merely different quantities of findings. The categories don't
converge; the coverage does. This suggests that multi-model evaluation
is a methodological requirement, not an optional enhancement.

The total cost of comprehensive cross-vendor analysis is twenty-seven
cents --- less than three minutes of minimum wage labor, less than a
single finding from a human security audit. The tools exist. The data is
clear. Nobody is checking.
