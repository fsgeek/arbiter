\section{Background and Related Work}

\subsection{Prompt Engineering}

The prompt engineering literature has grown rapidly since 2023, but
overwhelmingly focuses on crafting prompts for specific tasks: few-shot
formatting, chain-of-thought elicitation, retrieval-augmented generation
patterns. The quality of the prompt itself --- as a software artifact
with internal consistency requirements --- receives comparatively little
attention. Wei et al.~\cite{wei2022chain} established chain-of-thought
prompting; Yao et al.~\cite{yao2023tree} introduced tree-of-thoughts;
neither addresses the structural integrity of the prompts themselves.
Recent work has begun to frame prompts as first-class software
engineering artifacts requiring systematic management, evolution, and
testing~\cite{villamizar2025prompts}, but tools for structural analysis
of prompt internals remain absent.

\subsection{Prompt Security}

Prompt injection research~\cite{perez2022ignore,greshake2023not}
examines adversarial inputs designed to override system prompt
instructions. This work is orthogonal to ours: we analyze the system
prompt itself for internal contradictions, not external attacks against
it. A system prompt can be perfectly secure against injection yet
internally contradictory.

\subsection{Software Architecture Failure Modes}

The analogy between system prompt architectures and software
architectures is deliberate. Monolithic applications accumulate
contradictions at subsystem boundaries as teams add features
independently~\cite{conway1968committees}. Microservice architectures
push bugs to composition seams --- each service works internally, but
contracts between services may be
inconsistent~\cite{newman2015building}. Flat architectures
trade capability for simplicity. These same patterns appear in system
prompts, and for the same structural reasons.

\subsection{LLM-as-Judge}

Using LLMs to evaluate LLM outputs is established
practice~\cite{zheng2023judging}. Recent work has extended this to
multi-agent judge frameworks that aggregate multiple LLM evaluators for
improved robustness~\cite{yu2025agents,li2025metajudge}. We extend
multi-model evaluation in a different direction: where existing work
uses multiple judges to improve \emph{evaluation accuracy} (better
scores through consensus), Arbiter uses heterogeneous models to
deliberately surface \emph{different classes} of structural findings.
The goal is not consensus but complementarity: different models, trained
on different data, bring different analytical biases. A model trained
primarily on code (DeepSeek) notices structural issues that a model
trained on diverse text (Qwen) misses, and vice versa.
