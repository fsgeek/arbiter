\section{Background and Related Work}

\subsection{Prompt Engineering}

The prompt engineering literature has grown rapidly since 2023, but
overwhelmingly focuses on crafting prompts for specific tasks: few-shot
formatting, chain-of-thought elicitation, retrieval-augmented generation
patterns. The quality of the prompt itself --- as a software artifact
with internal consistency requirements --- receives comparatively little
attention. Wei et al.~(2022) established chain-of-thought prompting; Yao
et al.~(2023) introduced tree-of-thoughts; neither addresses the
structural integrity of the prompts themselves.

\subsection{Prompt Security}

Prompt injection research (Perez \& Ribeiro, 2022; Greshake et al.,
2023) examines adversarial inputs designed to override system prompt
instructions. This work is orthogonal to ours: we analyze the system
prompt itself for internal contradictions, not external attacks against
it. A system prompt can be perfectly secure against injection yet
internally contradictory.

\subsection{Software Architecture Failure Modes}

The analogy between system prompt architectures and software
architectures is deliberate. Monolithic applications accumulate
contradictions at subsystem boundaries as teams add features
independently (Conway, 1968). Microservice architectures push bugs to
composition seams --- each service works internally, but contracts
between services may be inconsistent (Newman, 2015). Flat architectures
trade capability for simplicity. These same patterns appear in system
prompts, and for the same structural reasons.

\subsection{LLM-as-Judge}

Using LLMs to evaluate LLM outputs is established practice (Zheng et
al., 2023). We extend this to multi-model evaluation, where the goal is
not consensus but complementarity: different models, trained on
different data, bring different analytical biases. A model trained
primarily on code (DeepSeek) notices structural issues that a model
trained on diverse text (Qwen) misses, and vice versa.
